import requests
import json
import time
import schedule
import random
import urllib.parse
from datetime import datetime

# ================= æ ¸å¿ƒé…ç½®åŒº (è¯·ä»”ç»†å¡«å†™) =================

# 1. ã€æœ€å…³é”®ä¸€æ­¥ã€‘è¯·æŠŠæµè§ˆå™¨é‡Œå¤åˆ¶çš„é‚£æ¡é•¿é“¾æ¥å®Œæ•´ç²˜è´´åœ¨ä¸‹é¢å¼•å·é‡Œï¼
#    (ä¹Ÿå°±æ˜¯å³é”® -> Copy link address å¾—åˆ°çš„é‚£ä¸€é•¿ä¸²)
#    ä»£ç ä¼šè‡ªåŠ¨ä»é‡Œé¢æå–æ­£ç¡®çš„ ID å’Œå‚æ•°ï¼Œä¸ç”¨ä½ æ‰‹åŠ¨æ”¹äº†ã€‚
Browser_Link = "https://x.com/i/api/graphql/M1jEez78PEfVfbQLvlWMvQ/SearchTimeline?variables=%7B%22rawQuery%22%3A%22from%3Alubi366%22%2C%22count%22%3A20%2C%22querySource%22%3A%22typed_query%22%2C%22product%22%3A%22Top%22%2C%22withGrokTranslatedBio%22%3Afalse%7D&features=%7B%22rweb_video_screen_enabled%22%3Afalse%2C%22profile_label_improvements_pcf_label_in_post_enabled%22%3Atrue%2C%22responsive_web_profile_redirect_enabled%22%3Afalse%2C%22rweb_tipjar_consumption_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Atrue%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22premium_content_api_read_enabled%22%3Afalse%2C%22communities_web_enable_tweet_community_results_fetch%22%3Atrue%2C%22c9s_tweet_anatomy_moderator_badge_enabled%22%3Atrue%2C%22responsive_web_grok_analyze_button_fetch_trends_enabled%22%3Afalse%2C%22responsive_web_grok_analyze_post_followups_enabled%22%3Atrue%2C%22responsive_web_jetfuel_frame%22%3Atrue%2C%22responsive_web_grok_share_attachment_enabled%22%3Atrue%2C%22articles_preview_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22responsive_web_twitter_article_tweet_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22responsive_web_grok_show_grok_translated_post%22%3Afalse%2C%22responsive_web_grok_analysis_button_from_backend%22%3Atrue%2C%22creator_subscriptions_quote_tweet_preview_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Atrue%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Atrue%2C%22longform_notetweets_rich_text_read_enabled%22%3Atrue%2C%22longform_notetweets_inline_media_enabled%22%3Atrue%2C%22responsive_web_grok_image_annotation_enabled%22%3Atrue%2C%22responsive_web_grok_imagine_annotation_enabled%22%3Atrue%2C%22responsive_web_grok_community_note_auto_translation_is_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%7D" 

# 2. ä½ çš„ Cookie (å¦‚æœæµè§ˆå™¨åˆ·æ–°è¿‡ï¼Œè¯·åŠ¡å¿…é‡æ–°æŠ“å–æ›´æ–°)
MY_AUTH_TOKEN = "c3778b43e1705ad15fd2e8b683087db33fb3aa1e"
MY_CT0 = "368af3c63dffcc690f8557421437270654944077c8fdd21103da457e4225508284c606385efa8dd6b74c5463e87eb42c0c91b68620b1e1827e0c8e8eb1db381efcc70fdce615e3d0351dc886b27b0cf0"

# 3. ç›‘æ§ç›®æ ‡
TARGET_ACCOUNTS = [
    "lubi366", "connectfarm1", "wolfyxbt", "Crypto_He", "BroLeon", 
    "0xcryptowizard", "one_snowball", "yueya_eth", "qlonline", 
    "ai_9684xtpa", "cz_binance", "linwanwan823"
]

# 4. n8n åœ°å€
N8N_WEBHOOK_URL = "http://43.139.245.223:5678/webhook/6d6ea3d6-ba16-4d9d-9145-22425474ab48"

# 5. æ—¶é—´è®¾ç½®
CHECK_INTERVAL_MINUTES = 16 
# =======================================================

last_seen_ids = {}

def get_base_url_and_features(full_url):
    """ä»é•¿é“¾æ¥ä¸­æå–å¹²å‡€çš„ URL æ¨¡æ¿"""
    try:
        parsed = urllib.parse.urlparse(full_url)
        # å¼ºåˆ¶ä½¿ç”¨ x.com
        base = f"https://x.com{parsed.path}"
        
        # è§£æå‚æ•°
        params = urllib.parse.parse_qs(parsed.query)
        
        # æå– features (æœ€å®¹æ˜“é”™çš„åœ°æ–¹ï¼Œç›´æ¥ç”¨æµè§ˆå™¨åŸç‰ˆçš„)
        features = params.get('features', [''])[0]
        
        return base, features
    except Exception as e:
        print(f"âŒ é“¾æ¥è§£æå¤±è´¥: {e}")
        return None, None

def get_latest_tweets():
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] === [1:1 å®Œç¾å¤åˆ»ç‰ˆ] å¼€å§‹æ£€æŸ¥ ===", flush=True)
    
    # è§£æç”¨æˆ·æä¾›çš„é•¿é“¾æ¥
    base_url, features_json = get_base_url_and_features(Browser_Link)
    
    if not base_url or not features_json:
        print("âŒ é”™è¯¯ï¼šè¯·ç¡®ä¿ä½ åœ¨ä»£ç é‡Œå¡«å…¥äº†æ­£ç¡®çš„ Browser_Link é•¿é“¾æ¥ï¼", flush=True)
        return

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Authorization": "Bearer AAAAAAAAAAAAAAAAAAAAANRILgAAAAAAnNwIzUejRCOuH5E6I8xnZz4puTs%3D1Zv7ttfk8LF81IUq16cHjhLTvJu4FA33AGWWjCpTnA",
        "Content-Type": "application/json",
        "X-Csrf-Token": MY_CT0,
        "x-twitter-active-user": "yes",
        "x-twitter-auth-type": "OAuth2Session",
        "x-twitter-client-language": "en",
        "Cookie": f"auth_token={MY_AUTH_TOKEN}; ct0={MY_CT0}"
    }

    for username in TARGET_ACCOUNTS:
        try:
            print(f"æ­£åœ¨æ£€æŸ¥: @{username} ...", end="", flush=True)
            
            # åŠ¨æ€æ„é€  variablesï¼Œä¿ç•™åŸç‰ˆæ‰€æœ‰å‚æ•°
            variables = {
                "rawQuery": f"from:{username}",
                "count": 5,
                "querySource": "typed_query",
                "product": "Latest"
            }
            
            # å‘é€è¯·æ±‚
            params = {
                "variables": json.dumps(variables),
                "features": features_json 
            }
            
            response = requests.get(base_url, headers=headers, params=params, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                try:
                    instructions = data['data']['search_by_raw_query']['search_timeline']['timeline']['instructions']
                    entries = []
                    for instr in instructions:
                        if instr['type'] == 'TimelineAddEntries':
                            entries = instr['entries']
                            break
                    
                    found_tweet = None
                    for entry in entries:
                        if 'tweet' in entry['entryId']: 
                            item_content = entry['content']['itemContent']['tweet_results']['result']
                            if 'legacy' in item_content:
                                found_tweet = item_content['legacy']
                            elif 'tweet' in item_content: 
                                found_tweet = item_content['tweet']['legacy']
                            if found_tweet:
                                break
                    
                    if found_tweet:
                        tweet_id = found_tweet['id_str']
                        full_text = found_tweet['full_text']
                        created_at = found_tweet['created_at']
                        
                        if username not in last_seen_ids:
                            last_seen_ids[username] = tweet_id
                            print(f" -> [åˆå§‹åŒ–] æœ€æ–° ID: {tweet_id}", flush=True)
                        elif last_seen_ids[username] != tweet_id:
                            print(f"\n  -> â˜… å‘ç°æ–°æ¨æ–‡ï¼æ¨é€ä¸­...", flush=True)
                            payload = {
                                "source": "twitter_monitor_auth",
                                "author": username,
                                "content_raw": full_text,
                                "link": f"https://x.com/{username}/status/{tweet_id}",
                                "tweet_id": tweet_id,
                                "timestamp": created_at
                            }
                            try:
                                requests.post(N8N_WEBHOOK_URL, json=payload, timeout=10)
                                print("  -> æ¨é€æˆåŠŸ âœ…", flush=True)
                                last_seen_ids[username] = tweet_id
                            except Exception as e:
                                print(f"  -> âŒ æ¨é€å¤±è´¥: {e}", flush=True)
                        else:
                            print(f" -> æ— æ›´æ–°", flush=True)
                    else:
                        print(" -> åˆ—è¡¨ä¸ºç©º (æ­£å¸¸)", flush=True)

                except Exception as e:
                    print(f" -> è§£æè·³è¿‡: {e}", flush=True)
            elif response.status_code == 404:
                print(" -> âŒ 404 é”™è¯¯ï¼è¯·æ£€æŸ¥ Browser_Link æ˜¯å¦å®Œæ•´å¤åˆ¶ï¼", flush=True)
                break 
            elif response.status_code == 401 or response.status_code == 403:
                print(" -> âŒ è®¤è¯å¤±è´¥ (Cookieå¤±æ•ˆï¼Œè¯·æ›´æ–°)", flush=True)
                break
            else:
                print(f" -> è¯·æ±‚å¤±è´¥: {response.status_code}", flush=True)

        except Exception as e:
            print(f" -> å¼‚å¸¸: {e}", flush=True)
            
        sleep_time = random.uniform(5, 8)
        time.sleep(sleep_time)

    print(f"=== æœ¬è½®ç»“æŸï¼Œç­‰å¾… {CHECK_INTERVAL_MINUTES} åˆ†é’Ÿ ===\n", flush=True)

if Browser_Link == "è¿™é‡Œç²˜è´´ä½ å¤åˆ¶çš„é•¿é“¾æ¥":
    print("âŒâŒâŒ è­¦å‘Šï¼šä½ è¿˜æ²¡æœ‰å¡«å…¥ Browser_Linkï¼è¯·å»æµè§ˆå™¨å¤åˆ¶ï¼âŒâŒâŒ")
else:
    print("ğŸ”¥ [System] å¯åŠ¨...", flush=True)
    get_latest_tweets()
    schedule.every(CHECK_INTERVAL_MINUTES).minutes.do(get_latest_tweets)

    while True:
        schedule.run_pending()
        time.sleep(1)
