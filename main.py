import requests
import json
import time
import schedule
import random
from bs4 import BeautifulSoup
from datetime import datetime

# ================= é…ç½®åŒº =================
TARGET_ACCOUNTS = [
    "lubi366", "connectfarm1", "wolfyxbt", "Crypto_He", "BroLeon", 
    "0xcryptowizard", "one_snowball", "yueya_eth", "qlonline", 
    "ai_9684xtpa", "cz_binance", "linwanwan823"
]

N8N_WEBHOOK_URL = "http://43.139.245.223:5678/webhook/6d6ea3d6-ba16-4d9d-9145-22425474ab48"

# å»ºè®®ç¨å¾®è°ƒé•¿ä¸€ç‚¹ï¼Œ15-20åˆ†é’Ÿï¼Œå¤ªé¢‘ç¹å®¹æ˜“è§¦å‘ Rate Limit
CHECK_INTERVAL_MINUTES = 20 

# éšæœº User-Agent æ± ï¼Œä¼ªè£…æˆä¸åŒè®¾å¤‡
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15"
]
# =========================================

last_seen_ids = {}

def get_latest_tweets():
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] === [å®˜æ–¹æ¥å£å¤æ´»ç‰ˆ] å¼€å§‹æ£€æŸ¥ ===", flush=True)
    
    for username in TARGET_ACCOUNTS:
        try:
            print(f"æ­£åœ¨æ£€æŸ¥: @{username} ...", end="", flush=True)
            
            # éšæœºå‚æ•° + å®˜æ–¹ syndication æ¥å£
            ts = int(time.time())
            url = f"https://syndication.twitter.com/srv/timeline-profile/screen-name/{username}?t={ts}"
            
            headers = {
                "User-Agent": random.choice(USER_AGENTS),
                "Referer": "https://twitter.com/",
                "Accept-Language": "en-US,en;q=0.9"
            }
            
            response = requests.get(url, headers=headers, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "html.parser")
                next_data = soup.find("script", {"id": "__NEXT_DATA__"})
                
                if next_data:
                    data = json.loads(next_data.string)
                    try:
                        # å®˜æ–¹è·¯å¾„æå–
                        entries = data['props']['pageProps']['timeline']['entries']
                        tweets = [e for e in entries if e['type'] == 'Tweet']
                        
                        if tweets:
                            latest_tweet = tweets[0]
                            content = latest_tweet['content']['tweet']
                            tweet_id = content['id_str']
                            tweet_text = content['text']
                            created_at = content['created_at'] # e.g., Thu Apr 06 15:28:43 +0000 2023
                            
                            # æ—¶é—´æ ¼å¼ç¾åŒ–
                            try:
                                dt = datetime.strptime(created_at, '%a %b %d %H:%M:%S +0000 %Y')
                                readable_time = dt.strftime('%Y-%m-%d %H:%M:%S')
                            except:
                                readable_time = created_at

                            # --- å¯¹æ¯”é€»è¾‘ ---
                            if username not in last_seen_ids:
                                last_seen_ids[username] = tweet_id
                                print(f" -> [åˆå§‹åŒ–] æœ€æ–° ID: {tweet_id}", flush=True)
                            
                            elif last_seen_ids[username] != tweet_id:
                                print(f"\n  -> â˜… å‘ç°æ–°æ¨æ–‡ï¼æ¨é€ä¸­...", flush=True)
                                
                                payload = {
                                    "source": "twitter_monitor_official",
                                    "author": username,
                                    "content_raw": tweet_text,
                                    "link": f"https://twitter.com/{username}/status/{tweet_id}",
                                    "tweet_id": tweet_id,
                                    "timestamp": readable_time
                                }
                                
                                try:
                                    requests.post(N8N_WEBHOOK_URL, json=payload, timeout=10)
                                    print("  -> æ¨é€æˆåŠŸ âœ…", flush=True)
                                    last_seen_ids[username] = tweet_id
                                except Exception as e:
                                    print(f"  -> âŒ æ¨é€å¤±è´¥: {e}", flush=True)
                            else:
                                print(f" -> æ— æ›´æ–° ({readable_time})", flush=True)
                        else:
                            print(" -> åˆ—è¡¨ä¸ºç©º", flush=True)
                    except Exception as e:
                        print(f" -> è§£æè·³è¿‡: {e}", flush=True)
                else:
                    print(" -> æœªæ‰¾åˆ°æ•°æ®æ ‡ç­¾", flush=True)
            elif response.status_code == 429:
                print(" -> âš ï¸ é™æµ (Rate Limit)ï¼Œä¼‘æ¯ä¸€ä¼š", flush=True)
            else:
                print(f" -> è®¿é—®å¤±è´¥: {response.status_code}", flush=True)

        except Exception as e:
            print(f" -> å¼‚å¸¸: {e}", flush=True)
            
        # å¢åŠ å»¶è¿Ÿï¼Œé˜²æ­¢è§¦å‘ 429
        time.sleep(random.uniform(10, 15))

    print(f"=== æœ¬è½®ç»“æŸï¼Œç­‰å¾… {CHECK_INTERVAL_MINUTES} åˆ†é’Ÿ ===\n", flush=True)

# å¯åŠ¨
print("ğŸ”¥ [System] æœºå™¨äººå·²å¤æ´»ï¼Œä½¿ç”¨å®˜æ–¹æ¥å£é€šé“...", flush=True)
get_latest_tweets()
schedule.every(CHECK_INTERVAL_MINUTES).minutes.do(get_latest_tweets)

while True:
    schedule.run_pending()
    time.sleep(1)
