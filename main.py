import requests
import json
import time
import schedule
import random
import urllib.parse
from datetime import datetime

# ================= æ ¸å¿ƒé…ç½®åŒº =================

# 1. ã€ç²˜è´´å¤„ã€‘è¯·å†æ¬¡ç²˜è´´é‚£æ¡é•¿é“¾æ¥ï¼ˆç¡®ä¿æ˜¯ Copy URL å¾—åˆ°çš„å®Œæ•´é“¾æ¥ï¼‰
#    è¿™ä¸€ç‰ˆä»£ç ä¼šè‡ªåŠ¨è¯»å–é“¾æ¥é‡Œçš„æ‰€æœ‰å‚æ•°ï¼Œä¸å†æ‰‹å†™ï¼Œé˜²æ­¢å‡ºé”™ã€‚
Browser_Link = "https://x.com/i/api/graphql/M1jEez78PEfVfbQLvlWMvQ/SearchTimeline?variables=%7B%22rawQuery%22%3A%22from%3Alubi366%22%2C%22count%22%3A20%2C%22querySource%22%3A%22typed_query%22%2C%22product%22%3A%22Top%22%2C%22withGrokTranslatedBio%22%3Afalse%7D&features=%7B%22rweb_video_screen_enabled%22%3Afalse%2C%22profile_label_improvements_pcf_label_in_post_enabled%22%3Atrue%2C%22responsive_web_profile_redirect_enabled%22%3Afalse%2C%22rweb_tipjar_consumption_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Atrue%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22premium_content_api_read_enabled%22%3Afalse%2C%22communities_web_enable_tweet_community_results_fetch%22%3Atrue%2C%22c9s_tweet_anatomy_moderator_badge_enabled%22%3Atrue%2C%22responsive_web_grok_analyze_button_fetch_trends_enabled%22%3Afalse%2C%22responsive_web_grok_analyze_post_followups_enabled%22%3Atrue%2C%22responsive_web_jetfuel_frame%22%3Atrue%2C%22responsive_web_grok_share_attachment_enabled%22%3Atrue%2C%22articles_preview_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22responsive_web_twitter_article_tweet_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22responsive_web_grok_show_grok_translated_post%22%3Afalse%2C%22responsive_web_grok_analysis_button_from_backend%22%3Atrue%2C%22creator_subscriptions_quote_tweet_preview_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Atrue%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Atrue%2C%22longform_notetweets_rich_text_read_enabled%22%3Atrue%2C%22longform_notetweets_inline_media_enabled%22%3Atrue%2C%22responsive_web_grok_image_annotation_enabled%22%3Atrue%2C%22responsive_web_grok_imagine_annotation_enabled%22%3Atrue%2C%22responsive_web_grok_community_note_auto_translation_is_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%7D" 

# 2. ä½ çš„ Cookie (å¦‚æœæµè§ˆå™¨åˆ·æ–°è¿‡ï¼Œå»ºè®®é‡æ–°å» Application æ çœ‹ä¸€çœ¼æœ‰æ²¡æœ‰å˜)
MY_AUTH_TOKEN = "c3778b43e1705ad15fd2e8b683087db33fb3aa1e"
MY_CT0 = "368af3c63dffcc690f8557421437270654944077c8fdd21103da457e4225508284c606385efa8dd6b74c5463e87eb42c0c91b68620b1e1827e0c8e8eb1db381efcc70fdce615e3d0351dc886b27b0cf0"

# 3. ç›‘æ§ç›®æ ‡
TARGET_ACCOUNTS = [
    "lubi366", "connectfarm1", "wolfyxbt", "Crypto_He", "BroLeon", 
    "0xcryptowizard", "one_snowball", "yueya_eth", "qlonline", 
    "ai_9684xtpa", "cz_binance", "linwanwan823"
]

# 4. n8n åœ°å€
N8N_WEBHOOK_URL = "http://43.139.245.223:5678/webhook/6d6ea3d6-ba16-4d9d-9145-22425474ab48"

# 5. æ—¶é—´è®¾ç½®
CHECK_INTERVAL_MINUTES = 16 
# ============================================

last_seen_ids = {}

def parse_browser_link(full_url):
    """
    æ·±åº¦è§£æé•¿é“¾æ¥ï¼Œæå–æ‰€æœ‰â€œåŸç”Ÿâ€å‚æ•°
    """
    try:
        parsed = urllib.parse.urlparse(full_url)
        # 1. æå–åŸºç¡€ URL (åŒ…å« ID)
        base_url = f"https://x.com{parsed.path}"
        
        # 2. æå–å‚æ•°
        qs = urllib.parse.parse_qs(parsed.query)
        
        # 3. æå– features (åŸæ ·ä¿ç•™)
        features_json = qs.get('features', ['{}'])[0]
        
        # 4. æå– variables (è¿™æ˜¯å…³é”®ï¼æˆ‘ä»¬è¦ç”¨å®ƒåšæ¨¡æ¿)
        variables_str = qs.get('variables', ['{}'])[0]
        variables_template = json.loads(variables_str)
        
        return base_url, features_json, variables_template
    except Exception as e:
        print(f"âŒ é“¾æ¥è§£æå¤±è´¥: {e}")
        return None, None, None

def get_latest_tweets():
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] === [å®Œå…¨å¯„ç”Ÿç‰ˆ] å¼€å§‹æ£€æŸ¥ ===", flush=True)
    
    # åŠ¨æ€è§£æé“¾æ¥
    base_url, features_json, variables_template = parse_browser_link(Browser_Link)
    
    if not base_url or not variables_template:
        print("âŒ é”™è¯¯ï¼šBrowser_Link è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ˜¯å¦å®Œæ•´ç²˜è´´ï¼", flush=True)
        return

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Authorization": "Bearer AAAAAAAAAAAAAAAAAAAAANRILgAAAAAAnNwIzUejRCOuH5E6I8xnZz4puTs%3D1Zv7ttfk8LF81IUq16cHjhLTvJu4FA33AGWWjCpTnA",
        "Content-Type": "application/json",
        "X-Csrf-Token": MY_CT0,
        "x-twitter-active-user": "yes",
        "x-twitter-auth-type": "OAuth2Session",
        "x-twitter-client-language": "en",
        "Cookie": f"auth_token={MY_AUTH_TOKEN}; ct0={MY_CT0}"
    }

    for username in TARGET_ACCOUNTS:
        try:
            print(f"æ­£åœ¨æ£€æŸ¥: @{username} ...", end="", flush=True)
            
            # === æ ¸å¿ƒé€»è¾‘ä¿®æ”¹ ===
            # æˆ‘ä»¬ä¸å†æ‰‹å†™ variablesï¼Œè€Œæ˜¯å¤åˆ¶ä¸€ä»½æµè§ˆå™¨çš„æ¨¡æ¿ï¼Œåªæ”¹ "rawQuery"
            current_variables = variables_template.copy()
            current_variables["rawQuery"] = f"from:{username}"
            
            # æ„é€ è¯·æ±‚å‚æ•°
            params = {
                "variables": json.dumps(current_variables),
                "features": features_json
            }
            
            # å‘é€è¯·æ±‚
            response = requests.get(base_url, headers=headers, params=params, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                try:
                    # å°è¯•è§£ææ¨æ–‡
                    instructions = data['data']['search_by_raw_query']['search_timeline']['timeline']['instructions']
                    entries = []
                    for instr in instructions:
                        if instr['type'] == 'TimelineAddEntries':
                            entries = instr['entries']
                            break
                    
                    found_tweet = None
                    for entry in entries:
                        if 'tweet' in entry['entryId']: 
                            item_content = entry['content']['itemContent']['tweet_results']['result']
                            if 'legacy' in item_content:
                                found_tweet = item_content['legacy']
                            elif 'tweet' in item_content: 
                                found_tweet = item_content['tweet']['legacy']
                            if found_tweet:
                                break
                    
                    if found_tweet:
                        tweet_id = found_tweet['id_str']
                        full_text = found_tweet['full_text']
                        created_at = found_tweet['created_at']
                        
                        if username not in last_seen_ids:
                            last_seen_ids[username] = tweet_id
                            print(f" -> [åˆå§‹åŒ–] æœ€æ–° ID: {tweet_id}", flush=True)
                        elif last_seen_ids[username] != tweet_id:
                            print(f"\n  -> â˜… å‘ç°æ–°æ¨æ–‡ï¼æ¨é€ä¸­...", flush=True)
                            payload = {
                                "source": "twitter_monitor_auth",
                                "author": username,
                                "content_raw": full_text,
                                "link": f"https://x.com/{username}/status/{tweet_id}",
                                "tweet_id": tweet_id,
                                "timestamp": created_at
                            }
                            try:
                                requests.post(N8N_WEBHOOK_URL, json=payload, timeout=10)
                                print("  -> æ¨é€æˆåŠŸ âœ…", flush=True)
                                last_seen_ids[username] = tweet_id
                            except Exception as e:
                                print(f"  -> âŒ æ¨é€å¤±è´¥: {e}", flush=True)
                        else:
                            print(f" -> æ— æ›´æ–°", flush=True)
                    else:
                        print(" -> åˆ—è¡¨ä¸ºç©º (æ­£å¸¸)", flush=True)

                except Exception as e:
                    # æœ‰æ—¶å€™æœç´¢ç»“æœä¸ºç©ºç»“æ„ä¼šä¸ä¸€æ ·ï¼Œå¿½ç•¥å³å¯
                    print(f" -> è§£æè·³è¿‡ (å¯èƒ½æ— ç»“æœ): {e}", flush=True)

            elif response.status_code == 404:
                print(" -> âŒ 404 é”™è¯¯ï¼", flush=True)
                # å¦‚æœè¿˜æ˜¯ 404ï¼Œå¤§æ¦‚ç‡æ˜¯ Cookie è¿‡æœŸäº†ï¼Œæˆ–è€…é“¾æ¥é‡Œçš„ variables è¿˜æ˜¯ä¸å¯¹
                break 
            elif response.status_code == 401 or response.status_code == 403:
                print(" -> âŒ è®¤è¯å¤±è´¥ (Cookie å¤±æ•ˆ)", flush=True)
                break
            else:
                print(f" -> è¯·æ±‚å¤±è´¥: {response.status_code}", flush=True)

        except Exception as e:
            print(f" -> å¼‚å¸¸: {e}", flush=True)
            
        sleep_time = random.uniform(5, 8)
        time.sleep(sleep_time)

    print(f"=== æœ¬è½®ç»“æŸï¼Œç­‰å¾… {CHECK_INTERVAL_MINUTES} åˆ†é’Ÿ ===\n", flush=True)

# æ£€æŸ¥æ˜¯å¦ç²˜è´´äº†é“¾æ¥
if "graphql" not in Browser_Link:
     print("âŒâŒâŒ è­¦å‘Šï¼šä½ è¿˜æ²¡æœ‰å¡«å…¥æ­£ç¡®çš„ Browser_Linkï¼è¯·å»æµè§ˆå™¨å¤åˆ¶ï¼âŒâŒâŒ")
else:
    print("ğŸ”¥ [System] å¯åŠ¨...", flush=True)
    get_latest_tweets()
    schedule.every(CHECK_INTERVAL_MINUTES).minutes.do(get_latest_tweets)

    while True:
        schedule.run_pending()
        time.sleep(1)
